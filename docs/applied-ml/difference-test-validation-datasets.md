# 测试和验证数据集之间有什么区别？

> 原文： [https://machinelearningmastery.com/difference-test-validation-datasets/](https://machinelearningmastery.com/difference-test-validation-datasets/)

验证数据集是训练模型时保留的数据样本，用于在调整模型的超参数时对模型技能进行估计。

验证数据集不同于测试数据集，该测试数据集也从模型的训练中退回，而是用于在最终模型之间进行比较或选择时给出最终调整模型的技能的无偏估计。

在应用机器学习中，关于验证数据集的确切内容以及它与测试数据集的不同之处存在很多混淆。

在这篇文章中，您将发现训练，测试和验证数据集的清晰定义，以及如何在您自己的机器学习项目中使用每个数据集。

阅读这篇文章后，你会知道：

*   机器学习领域的专家如何定义训练，测试和验证数据集。
*   实践中验证和测试数据集之间的区别。
*   在评估模型时，可用于充分利用验证和测试数据集的过程。

让我们开始吧。

![What is the Difference Between Test and Validation Datasets?](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/07/What-is-the-Difference-Between-Test-and-Validation-Datasets.jpg)

测试和验证数据集之间有什么区别？
[veddderman](https://www.flickr.com/photos/28055719@N02/3241773337/) 的照片，保留一些权利。

## 教程概述

本教程分为 4 个部分;他们是：

1.  什么是专家的验证数据集？
2.  训练，验证和测试数据集的定义
3.  验证数据集不够用
4.  验证和测试数据集消失

## 什么是专家的验证数据集？

我发现确切地看到从业者和专家如何描述数据集很有用。

在本节中，我们将根据一些顶级机器学习文本和参考资料，查看训练，测试和验证数据集的定义方式以及它们的区别。

通常，术语“_ 验证集 _”可与术语“_ 测试集 _”互换使用，并且是指从训练模型中阻止的数据集的样本。

对训练数据集的模型技能的评估将导致偏差得分。因此，在保留的样本上评估模型，以给出模型技能的无偏估计。这通常称为算法评估的训练测试分割方法。

> 假设我们想要估计与在一组观察上拟合特定统计学习方法相关的测试误差。验证集方法[...]是一项非常简单的策略。它涉及将可用的观察组随机分成两部分，即训练集和验证集或保持集。该模型适合训练集，拟合模型用于预测验证集中观察的响应。得到的验证集错误率 - 通常在定量响应的情况下使用 MSE 评估 - 提供测试错误率的估计。

- Gareth James 等，第 176 页，[统计学习导论：应用于 R](http://www.amazon.com/dp/1461471370?tag=inspiredalgor-20) ，2013。

我们可以直接在 Kuhn 和 Johnson 的优秀文本“Applied Predictive Modeling”中看到可互换性。在这个例子中，他们清楚地指出，最终的模型评估必须在先前未使用过的数据集上进行，用于训练模型或调整模型参数。

> 理想情况下，应该对未用于构建或微调模型的样本评估模型，以便它们提供无偏见的模型有效性。当手头有大量数据时，可以留出一组样本来评估最终模型。 “训练”数据集是用于创建模型的样本的通用术语，而“测试”或“验证”数据集用于限定表现。

- Max Kuhn 和 Kjell Johnson，第 67 页， [Applied Predictive Modeling](http://www.amazon.com/dp/1461468485?tag=inspiredalgor-20) ，2013

传统上，用于评估最终模型表现的数据集可称为“测试集”。 Russell 和 Norvig 在其开创性的 AI 教科书中重申了保持测试集完全分离的重要性。他们指的是以任何方式使用来自测试集的信息“偷看”。他们建议完全锁定测试装置，直到所有模型调整完成。

> 偷看是使用测试集表现来选择假设并对其进行评估的结果。避免这种情况的方法是真正保持测试集的锁定，直到你完全完成学习，并希望获得对最终假设的独立评估。 （然后，如果你不喜欢结果......如果你想回去找一个更好的假设，你必须获得并锁定一个全新的测试集。）

- Stuart Russell 和 Peter Norvig，第 709 页，[人工智能：现代方法](http://www.amazon.com/dp/0136042597?tag=inspiredalgor-20)，2009（第 3 版）

重要的是，Russell 和 Norvig 评论说，用于拟合模型的训练数据集可以进一步分为训练集和验证集，并且它是训练数据集的这个子集，称为验证集，可用于获取早期估计模型的技能。

> 如果测试集被锁定，但您仍希望测量未见数据的表现，作为选择良好假设的一种方法，则将可用数据（不包括测试集）划分为训练集和验证集。

- Stuart Russell 和 Peter Norvig，第 709 页，[人工智能：现代方法](http://www.amazon.com/dp/0136042597?tag=inspiredalgor-20)，2009（第 3 版）

验证集的这一定义得到了该领域其他开创性文本的证实。一个好的（和更老的）例子是 Ripley 的书“模式识别和神经网络”中的术语表。具体来说，训练，验证和测试集的定义如下：

> - 训练集：用于学习的一组示例，即适合分类器的参数。
> 
> - 验证集：用于调整分类器参数的一组示例，例如，用于选择神经网络中隐藏单元的数量。
> 
> - 测试集：一组仅用于评估完全指定分类器表现的示例。

- Brian Ripley，第 354 页，[模式识别和神经网络](http://www.amazon.com/dp/0521717701?tag=inspiredalgor-20)，1996

这些是这些术语的推荐定义和用法。

这些定义是规范的一个很好的例子是它们在着名的神经网络常见问题解答中的重复。除了重申 Ripley 的词汇表定义之外，它继续讨论在应用机器学习中常见的滥用术语“测试集”和“验证集”。

> 关于机器学习的文献经常颠倒“验证”和“测试”集的含义。这是人工智能研究中最常见的术语混淆的例子。
> 
> 关键的一点是，根据 NN [神经网络]文献中的标准定义，测试集从不用于在两个或更多网络中进行选择，因此测试集上的误差提供了对泛化误差的无偏估计（假设测试集代表人口等）。

- [主题：人口，样本，训练集，设计集，验证集和测试集是什么？](ftp://ftp.sas.com/pub/neural/FAQ.html#A_data)

您是否知道这些术语的任何其他明确定义或用法，例如：论文或教科书中的引用？
请在下面的评论中告诉我。

## 训练，验证和测试数据集的定义

为了重申研究上述专家的研究结果，本节提供了对这三个术语的明确定义。

*   **Training Dataset** ：用于拟合模型的数据样本。
*   **验证数据集**：用于在调整模型超参数的同时提供适合训练数据集的模型的无偏估计的数据样本。随着验证数据集上的技能被合并到模型配置中，评估变得更加偏向。
*   **测试数据集**：用于提供适合训练数据集的最终模型的无偏估计的数据样本。

我们可以使用伪代码草图制作这个具体内容：

```py
# split data
data = ...
train, validation, test = split(data)

# tune model hyperparameters
parameters = ...
for params in parameters:
	model = fit(train, params)
	skill = evaluate(model, validation)

# evaluate final model for comparison with other models
model = fit(train)
skill = evaluate(model, test)
```

以下是一些额外的澄清说明：

*   验证数据集还可以在其他形式的模型准备中发挥作用，例如特征选择。
*   最终模型可以适合训练和验证数据集的总和。

这些定义对您的用例是否清楚？
如果没有，请在下面提问。

## 验证数据集不够用

还有其他方法可以计算无偏见的（或在验证数据集的情况下逐渐更偏向）对未见数据的模型技能的估计。

一个流行的例子是使用 k-fold 交叉验证来调整模型超参数而不是单独的验证数据集。

在他们的书中，库恩和约翰逊有一个标题为“数据拆分建议”的部分，其中列出了使用唯一“测试集”（或验证集）的限制：

> 如前所述，针对单个独立测试集有一个强大的技术案例：
> 
> - 测试集是对模型的单一评估，并且对表征结果中的不确定性的能力有限。
> - 按比例大的测试集将数据划分为增加表现估计偏差的方式。
> - 样本量小：
> - 模型可能需要每个可能的数据点来充分确定模型值。
> - 测试集的不确定性可能相当大，以至于不同的测试集可能产生非常不同的结果。
> - 重采样方法可以合理预测模型在未来样本上的表现。

- Max Kuhn 和 Kjell Johnson，第 78 页， [Applied Predictive Modeling](http://www.amazon.com/dp/1461468485?tag=inspiredalgor-20) ，2013

由于表现估计的理想低偏差和方差特性，他们继续推荐使用 10 倍交叉验证的小样本量。在比较模型表现的情况下，他们建议使用自举法，因为表现估计的方差很小。

对于较大的样本量，他们再次建议采用 10 倍交叉验证方法。

## 验证和测试数据集消失

在现代应用机器学习中，您很可能不会看到对训练，验证和测试数据集的引用。

如果从业者选择使用与训练数据集的 k 折交叉验证来调整模型超参数，则对“验证数据集”的引用消失。

我们可以使用伪代码草图使这个具体如下：

```py
# split data
data = ...
train, test = split(data)

# tune model hyperparameters
parameters = ...
k = ...
for params in parameters:
	skills = list()
	for i in k:
		fold_train, fold_val = cv_split(i, k, train)
		model = fit(fold_train, params)
		skill_estimate = evaluate(model, fold_val)
		skills.append(skill_estimate)
	skill = summarize(skills)

# evaluate final model for comparison with other models
model = fit(train)
skill = evaluate(model, test)
```

如果使用训练数据集的模型超参数的交叉验证嵌套在模型的更广泛的交叉验证中，则对“测试数据集”的引用也可能消失。

最终，您剩下的就是来自域的数据样本，我们可能会继续将其称为训练数据集。

## 进一步阅读

如果您要深入了解，本节将提供有关该主题的更多资源。

*   维基百科上的[测试集](https://en.wikipedia.org/wiki/Test_set)
*   [主题：人口，样本，训练集，设计集，验证集和测试集是什么？](ftp://ftp.sas.com/pub/neural/FAQ.html#A_data) 神经网络常见问题解答
*   [统计学习简介：应用于 R](http://www.amazon.com/dp/1461471370?tag=inspiredalgor-20) ，2013
*   [Applied Predictive Modeling](http://www.amazon.com/dp/1461468485?tag=inspiredalgor-20) ，2013
*   [人工智能：现代方法](http://www.amazon.com/dp/0136042597?tag=inspiredalgor-20)，2009
*   [模式识别和神经网络](http://www.amazon.com/dp/0521717701?tag=inspiredalgor-20)，1996

你知道关于这个主题的任何其他好资源吗？请在下面的评论中告诉我。

## 摘要

在本教程中，您发现围绕术语“验证数据集”和“测试数据集”存在很多混淆，以及在评估您自己的机器学习模型的技能时如何正确导航这些术语。

具体来说，你学到了：

*   在评估模型时，“训练数据集”，“验证数据集”和“测试数据集”指的是明确的先例。
*   “验证数据集”主要用于描述调整超参数和数据准备时的模型评估，“测试数据集”主要用于描述最终调整模型在与其他最终模型进行比较时的评估。
*   当采用 k-fold 交叉验证等替代重采样方法时，“验证数据集”和“测试数据集”的概念可能会消失，尤其是在重采样方法嵌套时。

你有任何问题吗？
在下面的评论中提出您的问题，我会尽力回答。