# 机器学习的提升和AdaBoost

> 原文： [https://machinelearningmastery.com/boosting-and-adaboost-for-machine-learning/](https://machinelearningmastery.com/boosting-and-adaboost-for-machine-learning/)

Boosting是一种集合技术，试图从许多弱分类器中创建一个强分类器。

在这篇文章中，您将发现用于机器学习的AdaBoost Ensemble方法。阅读这篇文章后，你会知道：

*   增强集合方法是什么以及它的工作原理。
*   如何学习使用AdaBoost算法提升决策树。
*   如何使用学习过的AdaBoost模型做出预测。
*   如何最好地准备数据以与AdaBoost算法一起使用

这篇文章是为开发人员编写的，并没有统计或数学方面的背景。该文章重点介绍了算法的工作原理以及如何将其用于预测性建模问题。如果您有任何疑问，请发表评论，我会尽力回答。

让我们开始吧。

![Boosting and AdaBoost for Machine Learning](img/31a8a431e0d8a245c7f4e0736ccedb79.jpg)

用于机器学习的提升和AdaBoost
照片由 [KatieThebeau](https://www.flickr.com/photos/katiethebeau/8917329233/) 拍摄，保留一些权利。

## 提升集合方法

[Boosting](https://en.wikipedia.org/wiki/Boosting_(machine_learning)) 是一种通用的集合方法，可以从许多弱分类器中创建一个强分类器。

这是通过从训练数据构建模型，然后创建第二个模型来尝试从第一个模型中纠正错误来完成的。添加模型直到完美预测训练集或添加最大数量的模型。

[AdaBoost](https://en.wikipedia.org/wiki/AdaBoost) 是第一个为二分类开发的真正成功的增强算法。这是理解助力的最佳起点。

现代助推方法建立在AdaBoost上，最着名的是[随机梯度增强机](https://en.wikipedia.org/wiki/Gradient_boosting)。

## 获取免费算法思维导图

![Machine Learning Algorithms Mind Map](img/2ce1275c2a1cac30a9f4eea6edd42d61.jpg)

方便的机器学习算法思维导图的样本。

我已经创建了一个由类型组织的60多种算法的方便思维导图。

下载，打印并使用它。

## 从数据中学习AdaBoost模型

AdaBoost最适合用于提高决策树在二分类问题上的表现。

AdaBoost最初被Freund和Schapire技术的作者称为AdaBoost.M1。最近，它可能被称为离散AdaBoost，因为它用于分类而不是回归。

AdaBoost可用于提高任何机器学习算法的表现。它最适合弱学习器使用。这些模型在分类问题上实现了高于随机机会的准确率。

与AdaBoost一起使用的最适合且因此最常见的算法是具有一个级别的决策树。因为这些树很短，只包含一个分类决策，所以它们通常被称为决策树桩。

训练数据集中的每个实例都是加权的。初始权重设置为：

重量（xi）= 1 / n

其中xi是第i个训练实例，n是训练实例的数量。

## 如何训练一个模型

使用加权样本在训练数据上准备弱分类器（决策残余）。仅支持二进制（两类）分类问题，因此每个决策树桩对一个输入变量做出一个决策，并为第一个或第二个类值输出+1.0或-1.0值。

针对训练的模型计算错误分类率。传统上，这计算如下：

错误=（正确 - N）/ N.

如果错误是错误分类率，则正确的是模型正确预测的训练实例的数量，N是训练实例的总数。例如，如果模型正确预测了100个训练实例中的78个，则错误或错误分类率将为（78-100）/ 100或0.22。

这被修改为使用训练实例的权重：

error = sum（w（i）* terror（i））/ sum（w）

这是错误分类率的加权和，其中w是训练实例i的权重，而恐怖是训练实例i的预测误差，如果错误分类则为1，如果正确分类则为0。

例如，如果我们有3个训练实例，权重为0.01,0.5和0.2。预测值为-1，-1和-1，实例中的实际输出变量为-1,1和-1，然后误差为0,1和0.误分类率计算如下：

误差=（0.01 * 0 + 0.5 * 1 + 0.2 * 0）/（0.01 + 0.5 + 0.2）

要么

错误= 0.704

为训练模型计算阶段值，该阶段值为模型所做的任何预测提供加权。训练模型的阶段值计算如下：

stage = ln（（1-error）/ error）

其中stage是用于对模型进行加权预测的阶段值，ln（）是自然对数，而错误是模型的错误分类错误。阶段权重的影响是更准确的模型对最终预测具有更大的权重或贡献。

更新训练权重，为错误预测的实例提供更多权重，对正确预测的实例权重更小。

例如，使用以下内容更新一个训练实例（w）的权重：

w = w * exp（阶段*恐怖）

其中w是特定训练实例的权重，exp（）是数字常数e或欧拉数提升到幂，阶段是弱分类器的误分类率，恐怖是弱分类器预测输出变量的误差训练实例，评估为：

恐怖= 0如果（y == p），否则为1

其中y是训练实例的输出变量，p是来自弱学习器的预测。

如果训练实例被正确分类并且如果弱学习器错误地分类实例则使重量稍大，则这具有不改变重量的效果。

## AdaBoost Ensemble

按顺序添加弱模型，使用加权训练数据进行训练。

该过程一直持续到创建了预先设定数量的弱学习器（用户参数）或者不能对训练数据集进行进一步改进。

完成后，您将留下一群弱势学习器，每个学习器都有一个舞台值。

## 使用AdaBoost做出预测

通过计算弱分类器的加权平均值来做出预测。

对于新的输入实例，每个弱学习器计算预测值为+1.0或-1.0。预测值由每个弱学习器阶段值加权。集合模型的预测被视为加权预测的总和。如果总和为正，则预测第一类，如果为负，则预测第二类。

例如，5个弱分类器可以预测值1.0,1.0，-1.0,1.0，-1.0。从大多数投票来看，模型看起来预测值为1.0或第一类。这些相同的5个弱分类器可以分别具有阶段值0.2,0.5,0.8,0.2和0.9。计算这些预测的加权和导致输出为-0.8，这将是-1.0或第二类的集合预测。

## AdaBoost的数据准备

本节列出了为AdaBoost准备最佳数据的一些启发式方法。

*   **质量数据**：由于整体方法继续尝试纠正训练数据中的错误分类，因此您需要注意训练数据是高质量的。
*   **异常值**：异常值会迫使合奏团在兔子洞中努力工作以纠正不切实际的情况。这些可以从训练数据集中删除。
*   **噪声数据**：噪声数据，特别是输出变量中的噪声可能会有问题。如果可能，尝试从训练数据集中隔离和清除这些内容。

## 进一步阅读

以下是一些从机器学习角度描述AdaBoost的机器学习文本。

*   [统计学习简介：在R](http://www.amazon.com/dp/1461471370?tag=inspiredalgor-20) 中的应用，第321页
*   [统计学习要素：数据挖掘，推理和预测](http://www.amazon.com/dp/0387848576?tag=inspiredalgor-20)，第10章
*   [Applied Predictive Modeling](http://www.amazon.com/dp/1461468485?tag=inspiredalgor-20) ，第203页，第389页

下面是一些关于该方法的开创性和良好的概述研究文章，如果您希望深入研究该方法的理论基础，可能会有用：

*   [在线学习的决策理论推广及其应用](http://link.springer.com/chapter/10.1007/3-540-59119-2_166#page-1)，1995
*   [使用置信预测改进的Boosting算法](http://link.springer.com/article/10.1023/A:1007614523901)，1999
*   [解释Adaboost，来自经验推论的章节](http://link.springer.com/chapter/10.1007/978-3-642-41136-6_5)，2013
*   [Boosting简介](http://www.site.uottawa.ca/~stan/csi5387/boost-tut-ppr.pdf)，1999

## 摘要

在这篇文章中，您发现了用于机器学习的Boosting集合方法。你了解到：

*   提升以及它如何成为一种通用技术，不断增加弱学习器的正确分类错误。
*   AdaBoost是第一个成功的二分类问题的提升算法。
*   通过加权训练实例和弱学习器自己来学习AdaBoost模型。
*   通过加权来自弱学习器的预测来预测AdaBoost。
*   在哪里寻找有关AdaBoost算法的更多理论背景。

如果您对此帖子或Boosting或AdaBoost算法有任何疑问，请在评论中提出，我会尽力回答。