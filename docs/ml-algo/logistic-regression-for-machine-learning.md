# 机器学习的Logistic回归

> 原文： [https://machinelearningmastery.com/logistic-regression-for-machine-learning/](https://machinelearningmastery.com/logistic-regression-for-machine-learning/)

逻辑回归是统计领域机器学习所借用的另一种技术。

它是二元分类问题的首选方法（具有两个类值的问题）。在这篇文章中，您将发现用于机器学习的逻辑回归算法。

阅读这篇文章后你会知道：

*   描述逻辑回归时使用的许多名称和术语（如log odds和logit）。
*   用于逻辑回归模型的表示。
*   用于从数据中学习逻辑回归模型系数的技术。
*   如何使用学习的逻辑回归模型实际做出预测。
*   如果你想深入挖掘，可以去哪里获取更多信息。

本文是为对应用机器学习感兴趣的开发人员编写的，特别是预测建模。您不需要具有线性代数或统计数据的背景知识。

让我们开始吧。

![Learning Algorithm for Logistic Regression](img/aede623cb5a39de75438107c9e1b44ef.jpg)

Logistic回归的学习算法
摄影： [Michael Vadon](https://www.flickr.com/photos/80038275@N00/20459689318/) ，保留一些权利。

## 物流功能

逻辑回归以在该方法的核心使用的函数命名，即逻辑函数。

[逻辑函数](https://en.wikipedia.org/wiki/Logistic_function)，也称为sigmoid函数，由统计学家开发，用于描述生态中人口增长的特性，迅速上升并最大限度地提高环境的承载能力。它是一条S形曲线，可以取任何实数值并将其映射到0到1之间的值，但绝不会完全达到这些限制。

1 /（1 + e ^ - 值）

其中e是自然对数的[基数（电子表格中的欧拉数或EXP（）函数），value是您要转换的实际数值。下面是使用逻辑函数将-5和5之间的数字转换为0和1范围的图。](https://en.wikipedia.org/wiki/E_(mathematical_constant))

![Logistic Function](img/b27eff2d941d0d8e50b42686be5aaca9.jpg)

物流功能

现在我们知道了逻辑函数是什么，让我们看看它在逻辑回归中是如何使用的。

## 用于Logistic回归的表示法

Logistic回归使用方程作为表示，非常类似于线性回归。

使用权重或系数值（称为希腊大写字母Beta）线性组合输入值（x）以预测输出值（y）。与线性回归的主要区别在于，建模的输出值是二进制值（0或1）而不是数值。

以下是逻辑回归方程的示例：

y = e ^（b0 + b1 * x）/（1 + e ^（b0 + b1 * x））

其中y是预测输出，b0是偏差或截距项，b1是单个输入值（x）的系数。输入数据中的每一列都有一个相关的b系数（一个恒定的实际值），必须从训练数据中学习。

您将存储在存储器或文件中的模型的实际表示是等式中的系数（β值或b）。

## 获取免费算法思维导图

![Machine Learning Algorithms Mind Map](img/2ce1275c2a1cac30a9f4eea6edd42d61.jpg)

方便的机器学习算法思维导图的样本。

我已经创建了一个由类型组织的60多种算法的方便思维导图。

下载，打印并使用它。

## Logistic回归预测概率（技术插曲）

逻辑回归模拟默认类（例如第一类）的概率。

例如，如果我们从他们的身高模拟人的性别为男性或女性，那么第一类可能是男性，逻辑回归模型可以写成男性给予一个人身高的概率，或者更正式：

P（性别=男|高）

换句话说，我们正在模拟输入（X）属于默认类（Y = 1）的概率，我们可以正式地将其写为：

P（X）= P（Y = 1 | X）

我们预测概率？我认为逻辑回归是一种分类算法？

注意，必须将概率预测变换为二进制值（0或1）以便实际进行概率预测。稍后我们谈论做出预测时会有更多相关内容。

逻辑回归是一种线性方法，但使用逻辑函数转换预测。这样做的影响是我们不能再将预测理解为输入的线性组合，因为我们可以使用线性回归，例如，从上面继续，模型可以表示为：

p（X）= e ^（b0 + b1 * X）/（1 + e ^（b0 + b1 * X））

我不想过多地深入研究数学，但是我们可以如下转换上面的等式（记住我们可以通过向另一方添加自然对数（ln）来从一侧移除e）：

ln（p（X）/ 1-p（X））= b0 + b1 * X.

这很有用，因为我们可以看到右边的输出计算再次是线性的（就像线性回归一样），左边的输入是默认类概率的对数。

左边的这个比率被称为默认等级的几率（它是我们使用赔率的历史，例如，赔率用于赛马而不是概率）。赔率计算为事件概率除以事件概率的比率，例如： 0.8 /（1-0.8），赔率为4.所以我们可以写：

ln（赔率）= b0 + b1 * X.

因为赔率是对数转换的，所以我们称这个左侧为对数赔率或概率。可以使用其他类型的函数用于变换（其超出范围_，但是因此通常将关于线性回归方程与概率相关的变换称为链接函数，例如概率链接函数。

我们可以将指数向右移动并将其写为：

赔率= e ^（b0 + b1 * X）

所有这些都有助于我们理解模型确实仍然是输入的线性组合，但这种线性组合与默认类的对数几率相关。

## 学习Logistic回归模型

必须根据训练数据估算逻辑回归算法的系数（Beta值b）。这是使用最大似然估计来完成的。

[最大似然估计](https://en.wikipedia.org/wiki/Maximum_likelihood)是各种机器学习算法使用的常用学习算法，尽管它确实对数据的分布做出了假设（当我们谈论准备数据时更多关于此）。

最佳系数将导致模型预测默认类非常接近1的值（例如男性）和非常接近0的值（例如女性）用于另一类。逻辑回归的最大似然性的直觉是搜索过程寻求系数（Beta值）的值，其将模型预测的概率中的误差最小化到数据中的概率（例如，如果数据是主要的则概率为1）类）。

我们不打算进行最大可能性的计算。可以说最小化算法用于优化训练数据系数的最佳值。这通常在实践中使用有效的数值优化算法（如 [Quasi-newton方法](https://en.wikipedia.org/wiki/Quasi-Newton_method)）来实现。

当您学习逻辑时，您可以使用更简单的梯度下降算法从头开始实现它。

![Logistic Regression for Machine Learning](img/ce9bcd4ed68161d56dc06957b1db910d.jpg)

机器学习的逻辑回归
摄影： [woodleywonderworks](https://www.flickr.com/photos/wwworks/1430522839) ，保留一些权利。

## 利用Logistic回归做出预测

使用逻辑回归模型做出预测就像将数字插入逻辑回归方程并计算结果一样简单。

让我们通过一个具体的例子来具体化。

假设我们有一个模型可以根据他们的身高（完全是虚构的）来预测一个人是男性还是女性。鉴于身高150厘米的是男性或女性。

我们已经学习了系数b0 = -100和b1 = 0.6。使用上面的等式，我们可以计算出正式为150厘米或更高的男性P（男性身高= 150）的概率。我们将使用EXP（）作为e，因为如果您在电子表格中键入此示例，则可以使用此功能：

y = e ^（b0 + b1 * X）/（1 + e ^（b0 + b1 * X））

y = exp（-100 + 0.6 * 150）/（1 + EXP（-100 + 0.6 * X））

y = 0.0000453978687

或者该人是男性的概率几乎为零。

在实践中，我们可以直接使用概率。因为这是分类，我们想要一个清晰的答案，我们可以将概率捕捉到二进制类值，例如：

如果p（男性）&lt;0，则为0 0.5

如果p（男性）&gt; = 0.5，则为1

现在我们已经知道如何使用逻辑回归做出预测，让我们看看如何准备我们的数据以从技术中获得最大收益。

## 为Logistic回归准备数据

逻辑回归关于数据中的分布和关系的假设与线性回归中的假设大致相同。

许多研究已经用于定义这些假设，并使用精确的概率和统计语言。我的建议是使用这些作为指导或经验法则，并尝试不同的数据准备方案。

最终，在预测建模机器学习项目中，您将专注于进行准确的预测，而不是解释结果。因此，只要模型健壮且表现良好，您就可以打破一些假设。

*   **二进制输出变量**：这可能是显而易见的，因为我们已经提到它，但逻辑回归是针对二进制（两类）分类问题。它将预测属于默认类的实例的概率，可以将其捕捉到0或1分类中。
*   **去除噪音**：Logistic回归假设输出变量（y）没有错误，请考虑从训练数据中删除异常值和可能错误分类的实例。
*   **高斯分布**：逻辑回归是一种线性算法（对输出进行非线性变换）。它确实假设输入变量与输出之间存在线性关系。输入变量的数据变换可以更好地暴露这种线性关系，从而可以获得更准确的模型。例如，您可以使用log，root，Box-Cox和其他单变量转换来更好地公开此关系。
*   **删除相关输入**：与线性回归一样，如果您有多个高度相关的输入，模型可能会过拟合。考虑计算所有输入之间的成对相关性并去除高度相关的输入。
*   **未能收敛**：学习系数的预期似然估计过程有可能无法收敛。如果数据中存在许多高度相关的输入或数据非常稀疏（例如输入数据中存在大量零），则会发生这种情况。

## 进一步阅读

逻辑回归有很多可用的材料。它是生命科学和经济学等五门学科的最爱。

### Logistic回归资源

查看下面的一些书籍，了解有关逻辑回归算法的更多详细信息。

*   [广义线性模型，第二版](http://www.amazon.com/dp/0412317605?tag=inspiredalgor-20)
*   [Logistic回归：引物](http://www.amazon.com/dp/0761920102?tag=inspiredalgor-20)
*   [应用Logistic回归](http://www.amazon.com/dp/0470582472?tag=inspiredalgor-20)
*   [Logistic回归：自学文本](http://repository.cmu.edu/cgi/viewcontent.cgi?article=1217&context=robotics) [PDF]。

### 机器学习中的Logistic回归

对于机器学习焦点（例如仅在进行准确预测时），请查看下面一些流行机器学习文本中逻辑回归的覆盖范围：

*   [人工智能：现代方法（第3版）](http://www.amazon.com/dp/0136042597?tag=inspiredalgor-20)，第725-727页
*   [黑客机器学习](http://www.amazon.com/dp/1449303714?tag=inspiredalgor-20)，第178-182页
*   [统计学习简介：在R](http://www.amazon.com/dp/1461471370?tag=inspiredalgor-20) 中的应用，第130-137页
*   [统计学习要素：数据挖掘，推理和预测](http://www.amazon.com/dp/0387848576?tag=inspiredalgor-20)，第119-128页
*   [Applied Predictive Modeling](http://www.amazon.com/dp/1461468485?tag=inspiredalgor-20) ，第282-287页

如果我选择一个，我会指向[统计学习简介](http://www.amazon.com/dp/1461471370?tag=inspiredalgor-20)。这是一本很好的书。

## 摘要

在这篇文章中，您发现了用于机器学习和预测建模的逻辑回归算法。你涵盖了很多方面并且学到了：

*   逻辑函数是什么以及如何在逻辑回归中使用它。
*   逻辑回归中的关键表示是系数，就像线性回归一样。
*   使用称为最大似然估计的过程估计逻辑回归中的系数。
*   使用逻辑回归做出预测非常简单，您可以在Excel中做出预测。
*   逻辑回归的数据准备很像线性回归。

您对后退回归或此帖有任何疑问吗？
发表评论并问，我会尽力回答。