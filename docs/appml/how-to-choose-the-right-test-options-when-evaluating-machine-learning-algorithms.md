# 如何在评估机器学习算法时选择正确的测试选项

> 原文： [https://machinelearningmastery.com/how-to-choose-the-right-test-options-when-evaluating-machine-learning-algorithms/](https://machinelearningmastery.com/how-to-choose-the-right-test-options-when-evaluating-machine-learning-algorithms/)

当[评估机器学习算法](http://machinelearningmastery.com/how-to-evaluate-machine-learning-algorithms/ "How to Evaluate Machine Learning Algorithms")时使用的测试选项可能意味着过度学习，平庸的结果和可用的最先进结果之间的差异，您可以自信地从屋顶上呼喊（您真的觉得有时这样做）。

在本文中，您将发现可在算法评估测试工具中使用的标准测试选项，以及下次如何选择正确的选项。

## 随机性

选择正确的测试选项的困难的根源是随机性。大多数（几乎所有）机器学习算法都以某种方式使用随机性。随机性可以在算法中是明确的，或者可以在选择用于训练算法的数据的样本中。

[![Randomness](img/7fdef373b950726d311b892d6d8c33fb.jpg)](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2014/02/randomness.jpg)

随机性
摄影： [afoncubierta](http://www.flickr.com/photos/afoncubierta/12020857005/sizes/l/) ，保留一些权利

这并不意味着算法产生随机结果，这意味着它们产生具有一些噪声或[方差](http://en.wikipedia.org/wiki/Variance)的结果。我们称这种类型的有限方差，随机和利用它的算法，[随机算法](http://en.wikipedia.org/wiki/Stochastic_process)。

## 对相同数据进行训练和测试

如果您有数据集，则可能需要在数据集上训练模型，然后在该数据集上报告模型的结果。那个模型有多好，对吧？

这种评估算法的方法的问题在于，您确实会知道算法在数据集上的表现，但是没有任何关于算法将如何对未训练模型的数据执行的指示（所谓的看不见的数据） ）。

这很重要，只有当您想使用该模型对未见数据做出预测时。

## 分裂测试

使用一个数据集来训练和估计算法在看不见的数据上的表现的一种简单方法是拆分数据集。您获取数据集，并将其拆分为训练数据集和测试数据集。例如，您随机选择 66％的实例进行训练，并将剩余的 34％用作测试数据集。

该算法在训练数据集上运行，并在测试数据集上创建和评估模型，并获得表现准确度，即 87％的分类准确度。

当您拥有大量数据或训练模型时（特别是资源或时间），吐痰测试速度非常快。对非常大的数据集进行拆分测试可以准确估计算法的实际表现。

算法对数据有多好？我们可以自信地说它可以达到 87％的准确度吗？

问题是如果我们再次将训练数据集吐出到不同的 66％/ 34％分割中，我们将得到与我们的算法不同的结果。这称为模型方差。

## 多个拆分测试

分裂测试在数据集的不同拆分中获得不同结果的问题的解决方案是减少随机过程的方差并多次执行。我们可以从相当数量的运行中收集结果（比如 10）并取平均值。

例如，假设我们将数据集分成 66％/ 34％，运行我们的算法并得到准确度，我们用 10 次不同的分割完成了 10 次。我们可能有 10 个准确度分数如下：87,87,88,89,88,86,88,87,88,87。

我们模型的平均表现为 87.5，标准偏差约为 0.85。

[![Coin Toss](img/57a54ab84255096383635723c7b7e95b.jpg)](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2014/02/coin-toss.jpg)

硬币折腾
照片由 [ICMA 照片](http://www.flickr.com/photos/icma/3635981474/sizes/l/)，保留一些权利

多个拆分测试的一个问题是，某些数据实例可能永远不会包含在训练或测试中，而其他数据实例可能会多次选择。结果是，这可能会使结果产生偏差，并且可能无法对算法的准确率给出有意义的概念。

## 交叉验证

确保每个实例用于训练和测试相同次数同时减少准确度得分的方差的问题的解决方案是使用[交叉验证](http://en.wikipedia.org/wiki/Cross-validation_(statistics))。特别是 k 折交叉验证，其中 k 是数据集中要进行的拆分数。

例如，让我们选择 k = 10 的值（非常常见）。这将数据集分成 10 个部分（10 倍），算法将运行 10 次。每次运行算法时，它将在 90％的数据上进行训练，并在 10％的数据上进行测试，每次运行算法将改变算法测试的 10％的数据。

在此示例中，每个数据实例将恰好用作训练实例 9 次，并作为测试实例 1 次。准确度不是平均值和标准偏差，而是精确的准确度得分，表示进行了多少次正确的预测。

k 折交叉验证方法是用于评估算法在数据集上的表现的首选方法。您希望选择 k 值，为您的算法提供大小合适的训练和测试数据集。不太不成比例（对于训练或测试来说太大或太小）。如果您有大量数据，则可能不得不采用数据采样或恢复分割测试。

交叉验证确实可以对未见数据的算法表现进行无偏估计，但如果算法本身使用随机性会怎样。每次用不同的随机数种子（伪随机序列的开始）训练时，该算法将对相同的训练数据产生不同的结果。交叉验证不考虑算法预测的方差。

另一个值得关注的问题是交叉验证本身使用随机性来决定如何将数据集拆分为 k 个折叠。交叉验证不会估计算法如何使用不同的折叠集合执行。

这只有在您想要了解算法在数据集上的稳健性时才有意义。

## 多重交叉验证

考虑算法本身方差的一种方法是多次运行交叉验证，并从每次运行中获取算法精度的均值和标准差。

这将为您提供算法在数据集上的表现估计，以及对表现的稳健性（标准偏差的大小）的估计。

如果算法 A 有一个均值和标准差，算法 B 有另一个均值和标准差，并且它们不同（例如，算法 A 具有更高的准确度），您如何知道差异是否有意义？

这只有在您想比较算法之间的结果时才有意义。

## 统计学意义

当使用多次 k 次交叉验证时，比较算法表现测量的解决方案是使用[统计显着性](http://en.wikipedia.org/wiki/Statistical_hypothesis_testing)测试（如[T 检验](http://en.wikipedia.org/wiki/Student's_t-test)）。

多次运行 k 折交叉验证的结果是一个数字列表。我们希望使用均值和标准差来总结这些数字。您可以将这些数字视为来自基础人群的样本。统计显着性检验回答了这个问题：两个样本来自同一群体吗？ （没有不同）。如果答案是“是”，则即使平均值和标准偏差不同，也可以说差异在统计上不显着。

我们可以使用统计显着性检验来表示在使用多次运行时算法结果之间的差异（或缺乏差异）（如使用不同随机数种子的多次 k 次交叉验证）。这可以在我们想要对结果做出准确的声明时（算法 A 优于算法 B 且差异具有统计显着性）

这不是故事的结尾，因为这些测试（p 值）有不同的统计显着性检验（参数和非参数）和参数。我打算在这里画线，因为如果你已经跟我走了这么远，你现在已经足够了解选择测试选项以产生严格的（可发布的！）结果。

## 摘要

在这篇文章中，您发现了在设计测试工具以评估机器学习算法时可用的主要测试选项之间的区别。

具体来说，您了解了实用程序和问题：

*   对同一数据集进行训练和测试
*   拆分测试
*   多次拆分测试
*   交叉验证
*   多重交叉验证
*   统计学意义

如有疑问，请使用 k 折交叉验证（k = 10），并在想要有意义地比较数据集上的算法时，使用多次 k-fold 交叉验证和统计显着性检验。